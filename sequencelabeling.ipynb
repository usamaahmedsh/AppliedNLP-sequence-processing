{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Sequence Labeling\n",
    "\n",
    "In this assignment, you will work on the [MeasEval](https://competitions.codalab.org/competitions/25770) shared task that was part of SemEval-2021. The goal of **MeasEval** is  the extraction of counts, measurements, and related context from scientific documents. The task is a complex problem that involves solving a number of steps that range from identifying quantities and units of measurement to identify relationships between them. For this assignment, you will focus only on the *Quantity* recognition step: \n",
    "\n",
    "*  Given a paragraph from a scientific text, identify all spans containing quantities like *12 kg*. This problem can be approached as a Sequence Labeling task.\n",
    "\n",
    "You will develop a Recurrent Neural Network with [keras](https://keras.io/), a high-level Deep Learning API written in **Python** that provides a user-friendly interface for the [TensorFlow](https://www.tensorflow.org/) library, one of the most popular low-level Deep Learning frameworks. You will use the following objects and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Enable Eager mode to avoid issues with TimeDistributed layer in Keras 3\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When working with Neural Networks, there are a large number of random operations such as initializing the weights of the network, shuffling the data for training, or choosing samples. This causes that different training runs of the same model can lead to different results. To ensure reproducibility, i.e. obtaining the same results in the different runs, the random number generator must be initialized with a fixed value known as seed. In **keras**, this can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When developing a model, if the results you get are not as expected, try re-initializing the seed by running the cell above before compiling and training the model.\n",
    "\n",
    "> **Note!** With models as complex as Neural Networks, reproducibility is susceptible to factors such as software versions or the hardware on which the models are run. Even with seed initialization, there may be slight differences in the results.\n",
    "\n",
    "Working with Neural Networks also involves defining a number of hyperparameters that set the configuration of the model. Finding the appropriate hyperparameter values requires training the model with different combinations and testing them on the development set. This hyperparameter tuning is a costly process that needs multiple rounds of experimentation. However, for this assignments, you will use the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "maxlen = 130  # Maximum length of the input sequence accepted by the model\n",
    "epochs = 5  # Number of epochs to train the model\n",
    "batch_size = 64  # Number of examples used per gradient update\n",
    "embedding_dim = 300  # Dimension of the embeddings\n",
    "rnn_units = 256  # Number of units per RNN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Training a Deep Learning model with a large train set can be a time-consuming process, as the model needs to iterate over the entire set multiple times, often requiring significant computational resources. During the implementation of the model, it is often a good practice to use only a subset of the training data. This allows a faster debugging of the code. Set the `shrink_dataset` variable as `True` when a faster training is required and set it as `False` to train the model on the whole train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrink_dataset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Although the value of this variable does not affect the tests that will evaluate your code, the output examples distributed throughout this notebook are based on a `shrink_dataset` variable set as `False`.\n",
    "\n",
    "The train set for the assignment consists of 248 articles with 1366 sentences in total. The test set contains 136 articles with 848 sentences. A development set with 65 documents and 459 sentences is also provided. The dataset is annotated at the token level following a BIO schema with 3 labels: *B-Quantity*, *I-Quantity* and *O*.  The dataset can be loaded into three `DataFrames` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, shrink_dataset, seed):\n",
    "    data = pd.read_csv(data_path, sep=\"\\t\", encoding=\"utf8\").dropna()\n",
    "    if shrink_dataset:\n",
    "        sample = data[[\"docId\",  \"sentId\"]].drop_duplicates().sample(frac=0.2, random_state=seed)\n",
    "        data = pd.merge(data, sample, on=[\"docId\", \"sentId\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>sentId</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22276</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22277</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiment</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22278</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>involved</td>\n",
       "      <td>involve</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22279</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>B-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22280</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>beach</td>\n",
       "      <td>beach</td>\n",
       "      <td>I-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22281</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>materials</td>\n",
       "      <td>material</td>\n",
       "      <td>I-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22282</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22283</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>nominal</td>\n",
       "      <td>nominal</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22284</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>sediment</td>\n",
       "      <td>sediment</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22285</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>diameters</td>\n",
       "      <td>diameter</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22286</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22287</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>B-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22288</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>mm</td>\n",
       "      <td>mm</td>\n",
       "      <td>I-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22289</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>I-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22290</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>I-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22291</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>mm</td>\n",
       "      <td>mm</td>\n",
       "      <td>I-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22292</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        docId  sentId         word       lemma       label\n",
       "22276  S0378383912000130-3601       3          The         the           O\n",
       "22277  S0378383912000130-3601       3  experiments  experiment           O\n",
       "22278  S0378383912000130-3601       3     involved     involve           O\n",
       "22279  S0378383912000130-3601       3          two         two  B-Quantity\n",
       "22280  S0378383912000130-3601       3        beach       beach  I-Quantity\n",
       "22281  S0378383912000130-3601       3    materials    material  I-Quantity\n",
       "22282  S0378383912000130-3601       3         with        with           O\n",
       "22283  S0378383912000130-3601       3      nominal     nominal           O\n",
       "22284  S0378383912000130-3601       3     sediment    sediment           O\n",
       "22285  S0378383912000130-3601       3    diameters    diameter           O\n",
       "22286  S0378383912000130-3601       3           of          of           O\n",
       "22287  S0378383912000130-3601       3          1.5         1.5  B-Quantity\n",
       "22288  S0378383912000130-3601       3           mm          mm  I-Quantity\n",
       "22289  S0378383912000130-3601       3          and         and  I-Quantity\n",
       "22290  S0378383912000130-3601       3          8.5         8.5  I-Quantity\n",
       "22291  S0378383912000130-3601       3           mm          mm  I-Quantity\n",
       "22292  S0378383912000130-3601       3            .           .           O"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_data(\"data/train.tsv\", shrink_dataset, seed)\n",
    "dev_data = load_data(\"data/trial.tsv\", shrink_dataset, seed)\n",
    "test_data = load_data(\"data/eval.tsv\", shrink_dataset, seed)\n",
    "train_data[(train_data.docId == \"S0378383912000130-3601\") & (train_data.sentId == 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The `DataFrames` created include the lemmatization of words in the `lemma` columns. You will use the lemmas as the input of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "In this assignment, you will have to implement some steps to pre-process and obtain a representation of the data. You will implement a model with an `Embedding` lookup table as the input layer, so the tokens of the input sentences should be represented as indexes. The target labels should also be represented in similar way. Besides, as one would expect, the sentences in the **MeasEval** dataset have different lengths. However, the input for a Deep Learning model is a batch of examples (in this case, sentences) in the form of a single tensor which requires that all examples in the batch must have the same length. Therefore, the sentences should be padded or truncated to a specific length.\n",
    "\n",
    "> **Note!** For this particular task, the `maxlen` value provided to you guarantees that padding is sufficient to make all sentences the same length without the need for truncation. \n",
    "\n",
    "This first of these pre-processing steps will be to obtain both a vocabulary and the set of labels from the train set. The vocabulary should be the list of unique lemmas and must include the special tokens `[PAD]`, that will be used for padding the sequences, and `[UNK]`, that will be used to represent out-of-vocabulary words. Along with the vocabulary and the label set, you will also have to build a dictionary mapping each lemma to its position in the vocabulary and a dictionary mapping each label to its position in the label set. These dictionaries will be used later to obtain the representation of the input and output of the model. The text is already tokenized and lemmatized which will help in this task.\n",
    "\n",
    "You must complete the code for the `get_vocabulary` function that takes as input the `DataFrame` containing the train set. The function should create a list with the all the unique lemmas and include the special tokens `[PAD]` and `[UNK]` in the first two positions. Similarly, the function should create a list with the unique labels with the special token `[PAD]` in the first position. The **pandas** library provides some [functions](https://pandas.pydata.org/docs/reference/index.html) that may help you. Along with those lists, `get_vocabulary` should return the dictionaries mapping the lemmas and the labels to their corresponding positions. In total, the vocabulary and the label set should have 5508 and 4 items respectively:\n",
    "\n",
    "> Vocabulary size: 5508  \n",
    "Vocabulary first 5 lemmas: ['[PAD]', '[UNK]', 'datum', 'be', 'draw']  \n",
    "Vocabulary dictionary: {'[PAD]': 0, '[UNK]': 1, 'datum': 2, 'be': 3, 'draw': 4}  \n",
    ">\n",
    ">Labels size: 4  \n",
    "Labels: ['[PAD]', 'O', 'B-Quantity', 'I-Quantity']  \n",
    "Labels dictionary: {'[PAD]': 0, 'O': 1, 'B-Quantity': 2, 'I-Quantity': 3}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "get_vocabulary"
    ]
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(train_data):\n",
    "   # Initialize lists for lemmas and Labels\n",
    "    vocab = ['[PAD]', '[UNK]']\n",
    "    labels = ['[PAD]']\n",
    "\n",
    "    # Initialize dictionaries to map lemmas and labels to positions\n",
    "    word2idx = {'[PAD]': 0, '[UNK]': 1}\n",
    "    label2idx = {'[PAD]': 0}\n",
    "\n",
    "    # Initialize lists for unique lemmas and labels as lists\n",
    "    distinct_lemmas = list(train_data['lemma'].unique())\n",
    "    distinct_labels = list(train_data['label'].unique())\n",
    "\n",
    "    vocab.extend(distinct_lemmas)\n",
    "    labels.extend(distinct_labels)\n",
    "\n",
    "    # Create dictionaries to map lemmas and labels to positions\n",
    "    word2idx.update({lemma: idx for idx, lemma in enumerate(vocab)})\n",
    "    label2idx.update({label: idx for idx, label in enumerate(labels)})\n",
    "\n",
    "    return vocab, word2idx, labels, label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5508\n",
      "Vocabulary first 5 words: ['[PAD]', '[UNK]', 'datum', 'be', 'draw']\n",
      "Vocabulary dictionary: {'[PAD]': 0, '[UNK]': 1, 'datum': 2, 'be': 3, 'draw': 4}\n",
      "\n",
      "Labels size: 4\n",
      "Labels: ['[PAD]', 'O', 'B-Quantity', 'I-Quantity']\n",
      "Labels dictionary: {'[PAD]': 0, 'O': 1, 'B-Quantity': 2, 'I-Quantity': 3}\n"
     ]
    }
   ],
   "source": [
    "vocab, word2idx, labels, label2idx = get_vocabulary(train_data)\n",
    "vocab_size = len(vocab)\n",
    "label_size = len(labels)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary first 5 words: {vocab[:5]}\")\n",
    "print(f\"Vocabulary dictionary: { {w: word2idx[w] for w in vocab[:5]}}\")\n",
    "print(\"\")\n",
    "print(f\"Labels size: {label_size}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Labels dictionary: {label2idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Since the *Quantity* recognition task is a Sequence Labeling problem, the input for the model must be the sequence of lemmas in the sentence and the output the sequence of labels. Therefore, the train, development and test `DataFrames` must be reformated by aggregating the data corresponding to each sentence. The `integrate_sentences` will do this for you. The output of `integrate_sentences` is a `DataFrame` with a row for each sentence and the columns `lemmas` and `labels` that contain the list of lemmas and the list of labels of the sentences respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "sentence_integrate"
    ]
   },
   "outputs": [],
   "source": [
    "def integrate_sentences(data):\n",
    "    agg_func = lambda s: [s['lemma'].values.tolist(), s['label'].values.tolist()]\n",
    "    data = data.groupby([\"docId\", \"sentId\"], sort=False).apply(agg_func, include_groups=False).reset_index().rename(columns={0: 'lemmas_labels'})\n",
    "    data['lemmas'] = data.apply(lambda x: x['lemmas_labels'][0], axis=1)\n",
    "    data['labels'] = data.apply(lambda x: x['lemmas_labels'][1], axis=1)\n",
    "    data = data.drop(columns=\"lemmas_labels\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>sentId</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>[the, experiment, involve, two, beach, material, with, nominal, sediment, diameter, of, 1.5, mm, and, 8.5, mm, .]</td>\n",
       "      <td>[O, O, O, B-Quantity, I-Quantity, I-Quantity, O, O, O, O, O, B-Quantity, I-Quantity, I-Quantity, I-Quantity, I-Quantity, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docId  sentId  \\\n",
       "767  S0378383912000130-3601       3   \n",
       "\n",
       "                                                                                                                lemmas  \\\n",
       "767  [the, experiment, involve, two, beach, material, with, nominal, sediment, diameter, of, 1.5, mm, and, 8.5, mm, .]   \n",
       "\n",
       "                                                                                                                          labels  \n",
       "767  [O, O, O, B-Quantity, I-Quantity, I-Quantity, O, O, O, O, O, B-Quantity, I-Quantity, I-Quantity, I-Quantity, I-Quantity, O]  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples = integrate_sentences(train_data)\n",
    "dev_examples = integrate_sentences(dev_data)\n",
    "test_examples = integrate_sentences(test_data)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "train_examples[(train_examples.docId == \"S0378383912000130-3601\") & (train_examples.sentId == 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now ready for you to get the numerical representation of both input and output. You must perform two steps to process the sequence of lemmas and the sequence of labels:\n",
    "1. For each sentence, translate each lemma or label to its corresponding index using the `word2idx` and `label2idx` dictionaries. In case the lemma is not found in `word2idx`, use the index of the `[UNK]` token instead.\n",
    "2. Pad both the sequences of lemmas and the sequences of labels to the same length as defined by the `maxlen` variable. For this, you should use the [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) function with its default padding strategy. This function uses `0` as the default padding value which corresponds to the index of the `[PAD]` token in the vocabulary.   \n",
    "\n",
    "You must complete the code for the `format_examples` function. This function takes as input a `DataFrame` in the format returned by `integrate_sentences`, the `word2idx` and `label2idx` dictionaries, and the `maxlen` variable. The function must run the steps described above and return a **numpy** array with the processed lemma sequences and a **numpy** array with the processed label sequences that will be used as input and output of the model respectively. Applying `format_examples` to the train, development and test sets should result on 6 arrays with the following shapes:\n",
    "\n",
    ">Shape of train input data :  (1366, 130)  \n",
    "Shape of train output data :  (1366, 130)  \n",
    "Shape of development input data :  (459, 130)  \n",
    "Shape of development output data :  (459, 130)  \n",
    "Shape of test input data :  (848, 130)  \n",
    "Shape of test output data :  (848, 130)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "format_examples"
    ]
   },
   "outputs": [],
   "source": [
    "def format_examples(data, word2idx, label2idx, maxlen):\n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for lemmas, labels in zip(data['lemmas'], data['labels']):\n",
    "        lemma_indices = [word2idx.get(lemma, word2idx['[UNK]']) for lemma in lemmas]\n",
    "        label_indices = [label2idx[label] for label in labels]\n",
    "        \n",
    "        x_data.append(lemma_indices)\n",
    "        y_data.append(label_indices)\n",
    "\n",
    "    x_data = pad_sequences(x_data, maxlen= maxlen)\n",
    "    y_data = pad_sequences(y_data, maxlen= maxlen)\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train input data:  (1366, 130)\n",
      "Shape of train output data:  (1366, 130)\n",
      "Shape of development input data:  (459, 130)\n",
      "Shape of development output data:  (459, 130)\n",
      "Shape of train input data:  (848, 130)\n",
      "Shape of test output data:  (848, 130)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = format_examples(train_examples, word2idx, label2idx, maxlen)\n",
    "x_dev, y_dev = format_examples(dev_examples, word2idx, label2idx, maxlen)\n",
    "x_test, y_test = format_examples(test_examples, word2idx, label2idx, maxlen)\n",
    "print(\"Shape of train input data: \", x_train.shape)\n",
    "print(\"Shape of train output data: \", y_train.shape)\n",
    "print(\"Shape of development input data: \", x_dev.shape)\n",
    "print(\"Shape of development output data: \", y_dev.shape)\n",
    "print(\"Shape of train input data: \", x_test.shape)\n",
    "print(\"Shape of test output data: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "There are three ways to create a neural network with **keras**: using the [Functional API](https://keras.io/guides/functional_api/), by [Model subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) or creating a [Sequential model](https://keras.io/guides/sequential_model/). In this assignment, you will use the latter option. A `Sequential` model is a straightforward approach to build simple neural networks by stacking the layers. You will construct a RNN with the following 3 layers:\n",
    "1. An [Input](https://keras.io/api/layers/core_layers/input/) layer where the length of the input sequences is set to `maxlen`.\n",
    "2. An [Embedding](https://keras.io/api/layers/core_layers/embedding/) layer with an input dimension equal to the vocabulary size and an embedding dimension defined by `embedding_dim`. The layer must also mask out the padding values so that they are not considered when computing the loss.\n",
    "3. A Bidirectional [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/) layer with a number of units determined by `rnn_units`. Since you are working on Sequence Labeling, the `LSTM` must return outputs for the full sequence. To make it Bidirectional, the `LSTM` must be wrapped by a [Bidirectional](https://keras.io/api/layers/recurrent_layers/bidirectional/) layer.\n",
    "4. A [Dense](https://keras.io/api/layers/core_layers/dense/) layer with a number of units equal to the number of labels and a `softmax` activation function.  Since you are working on Sequence Labeling, the `Dense` layer must be wrapped by a [TimeDistributed](https://keras.io/api/layers/recurrent_layers/time_distributed/) layer.\n",
    "\n",
    "You must complete the code for the `create_model` function. This function takes as input the size of the vocabulary, the number of labels and the `maxlen`, `embedding_dim` and `rnn_units` hyperparameters. The function must create a RNN according to the configuration described above. Read carefully all the linked documentation to learn how to create such a model.  Any option not mentioned in the description should be kept with its default value. The summary of the resulting model should look like:\n",
    "\n",
    "\n",
    "> <pre>\n",
    "> Model: \"sequential_1\"\n",
    "> __________________________________________________________________________________________\n",
    "> Layer (type)                           Output Shape                        Param #       \n",
    "> ==========================================================================================\n",
    "> embedding_1 (Embedding)               (None, 130, 300)                    1652400       \n",
    ">                                                                                          \n",
    "> bidirectional_1 (Bidirectional)       (None, 130, 512)                    1140736       \n",
    ">                                                                                          \n",
    "> time_distributed_1 (TimeDistributed)  (None, 130, 4)                      2052          \n",
    ">                                                                                          \n",
    "> ==========================================================================================\n",
    "> Total params: 2,795,188\n",
    "> Trainable params: 2,795,188\n",
    "> Non-trainable params: 0\n",
    "> __________________________________________________________________________________________\n",
    "> </pre>\n",
    "\n",
    "Before returning the model, the `create_model` function should [compile](https://keras.io/api/models/model_training_apis/#compile-method) it using `'sparse_categorical_crossentropy'` as the loss function, `'adam'` as the optimizer and `'sparse_categorical_accuracy'` as a metric to evaluate the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": [
     "create_model"
    ]
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, label_size, maxlen, embedding_dim, rnn_units):\n",
    "   # Define Model \n",
    "    model = tf.keras.Sequential([\n",
    "        Input(shape=(maxlen,)),\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True),\n",
    "        Bidirectional(LSTM(units = rnn_units, return_sequences=True)),\n",
    "        TimeDistributed(Dense(units = label_size, activation = 'softmax'))\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['sparse_categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                          </span>┃<span style=\"font-weight: bold\"> Output Shape                 </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,652,400</span> │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,140,736</span> │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ time_distributed_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,052</span> │\n",
       "└───────────────────────────────────────┴──────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_16 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │       \u001b[38;5;34m1,652,400\u001b[0m │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_15 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │       \u001b[38;5;34m1,140,736\u001b[0m │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ time_distributed_14 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m4\u001b[0m)               │           \u001b[38;5;34m2,052\u001b[0m │\n",
       "└───────────────────────────────────────┴──────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,795,188</span> (10.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,795,188\u001b[0m (10.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,795,188</span> (10.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,795,188\u001b[0m (10.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_model(vocab_size, label_size, maxlen, embedding_dim, rnn_units)\n",
    "model.summary(line_length=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Once the data has been processed and the model has been compiled, you can proceed to train it. \n",
    "\n",
    "You must complete the `train_model` function. The function takes as input the model created by `create_model`, the train input and output obtained by `format_examples` as well as the development input and output produced by the same function. The function also takes the `batch_size` and `epochs` hyperparameters. The function should train the model on the training data using those hyperparameters. During the training, `train_model` should evaluate the loss and any model metrics on the development data. With `shrink_dataset = False`, the training will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": [
     "train_model"
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_dev, y_dev, batch_size, epochs):\n",
    "     \n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_dev, y_dev),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 6s/step - loss: 0.8155 - sparse_categorical_accuracy: 0.3080 - val_loss: 0.2391 - val_sparse_categorical_accuracy: 0.2154\n",
      "Epoch 2/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 6s/step - loss: 0.2304 - sparse_categorical_accuracy: 0.2043 - val_loss: 0.1824 - val_sparse_categorical_accuracy: 0.2160\n",
      "Epoch 3/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 5s/step - loss: 0.1509 - sparse_categorical_accuracy: 0.2081 - val_loss: 0.1310 - val_sparse_categorical_accuracy: 0.2191\n",
      "Epoch 4/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 5s/step - loss: 0.1039 - sparse_categorical_accuracy: 0.2067 - val_loss: 0.1109 - val_sparse_categorical_accuracy: 0.2205\n",
      "Epoch 5/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 5s/step - loss: 0.0674 - sparse_categorical_accuracy: 0.2104 - val_loss: 0.0975 - val_sparse_categorical_accuracy: 0.2215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x218385b8b60>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, x_train, y_train, x_dev, y_dev, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "After training, the model can be used to make predictions on unlabeled data using the [predict](https://keras.io/api/models/model_training_apis/#predict-method) method.\n",
    "\n",
    "You must complete the code for the `make_predictions` function. The functions takes as input the model already trained, the test input data produced by `format_examples` and the `batch_size` hyperparameter. The function must run the `predict` method on the input data using batches of size equal to `batch_size`. The `predict` method will return a **numpy** array with 3 axes: `(number of sentences, maxlen, label_size)`. For each token in each sentence, `predict` returns a vector with the probabilities predicted for every label. The output of `make_predictions` must include only the index of the label with the highest probability for each token. For example, if the prediction for one token is the vector `[0.04974193, 0.1511916, 0.65180656, 0.14725993]`, the output for that token should be `2`. For this, you can apply the [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) method along the last axis of the **numpy** array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": [
     "make_predictions"
    ]
   },
   "outputs": [],
   "source": [
    "def make_predictions(model, x_test, batch_size):\n",
    "        \n",
    "    pred = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "    pred_labels = np.argmax(pred, axis=-1)\n",
    "\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "predictions = make_predictions(model, x_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Since the predictions are now label indexes, they can be translated to the corresponding label by accessing the `labels` list. The `predictions_to_labels` functions iterates over all the sequences in the test set and translates the prediction of each token to the corresponding label. The function skips the padding tokens. The new format of the predictions can be stored in the `prediction` column of the test `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_labels(predictions, x_test, labels):\n",
    "    pred_labels = []\n",
    "    for pred_seq, x_seq in zip(predictions, x_test):\n",
    "        pred_seq_labels = [labels[p] for p, x in zip(pred_seq, x_seq) if x!=0]\n",
    "        pred_labels.extend(pred_seq_labels)\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>sentId</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7328</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>Approximately</td>\n",
       "      <td>approximately</td>\n",
       "      <td>B-Quantity</td>\n",
       "      <td>B-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7329</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>I-Quantity</td>\n",
       "      <td>B-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7330</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>min</td>\n",
       "      <td>min</td>\n",
       "      <td>I-Quantity</td>\n",
       "      <td>I-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7331</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>before</td>\n",
       "      <td>before</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7333</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>beginning</td>\n",
       "      <td>beginning</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7334</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7335</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7336</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>experiment</td>\n",
       "      <td>experiment</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7337</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7338</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7339</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>aboveground</td>\n",
       "      <td>aboveground</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7340</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>vegetation</td>\n",
       "      <td>vegetation</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7341</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7342</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>removed</td>\n",
       "      <td>remove</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7343</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>under</td>\n",
       "      <td>under</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7344</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7345</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>dual</td>\n",
       "      <td>dual</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7346</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>chamber</td>\n",
       "      <td>chamber</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>well</td>\n",
       "      <td>well</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7352</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7353</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>reference</td>\n",
       "      <td>reference</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7354</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>chamber</td>\n",
       "      <td>chamber</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7355</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>which</td>\n",
       "      <td>which</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7356</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7357</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>deployed</td>\n",
       "      <td>deploy</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7358</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7359</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>B-Quantity</td>\n",
       "      <td>B-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7360</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>cm</td>\n",
       "      <td>cm</td>\n",
       "      <td>I-Quantity</td>\n",
       "      <td>I-Quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7361</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>distance</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7362</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7363</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7364</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>dual</td>\n",
       "      <td>dual</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7365</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7366</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>chamber</td>\n",
       "      <td>chamber</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7367</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       docId  sentId           word          lemma  \\\n",
       "7328  S0038071711004354-1624       2  Approximately  approximately   \n",
       "7329  S0038071711004354-1624       2             15             15   \n",
       "7330  S0038071711004354-1624       2            min            min   \n",
       "7331  S0038071711004354-1624       2         before         before   \n",
       "7332  S0038071711004354-1624       2            the            the   \n",
       "7333  S0038071711004354-1624       2      beginning      beginning   \n",
       "7334  S0038071711004354-1624       2             of             of   \n",
       "7335  S0038071711004354-1624       2            the            the   \n",
       "7336  S0038071711004354-1624       2     experiment     experiment   \n",
       "7337  S0038071711004354-1624       2              ,              ,   \n",
       "7338  S0038071711004354-1624       2            the            the   \n",
       "7339  S0038071711004354-1624       2    aboveground    aboveground   \n",
       "7340  S0038071711004354-1624       2     vegetation     vegetation   \n",
       "7341  S0038071711004354-1624       2            was             be   \n",
       "7342  S0038071711004354-1624       2        removed         remove   \n",
       "7343  S0038071711004354-1624       2          under          under   \n",
       "7344  S0038071711004354-1624       2            the            the   \n",
       "7345  S0038071711004354-1624       2           dual           dual   \n",
       "7346  S0038071711004354-1624       2              -              -   \n",
       "7347  S0038071711004354-1624       2        chamber        chamber   \n",
       "7348  S0038071711004354-1624       2              ,              ,   \n",
       "7349  S0038071711004354-1624       2             as             as   \n",
       "7350  S0038071711004354-1624       2           well           well   \n",
       "7351  S0038071711004354-1624       2             as             as   \n",
       "7352  S0038071711004354-1624       2            the            the   \n",
       "7353  S0038071711004354-1624       2      reference      reference   \n",
       "7354  S0038071711004354-1624       2        chamber        chamber   \n",
       "7355  S0038071711004354-1624       2          which          which   \n",
       "7356  S0038071711004354-1624       2            was             be   \n",
       "7357  S0038071711004354-1624       2       deployed         deploy   \n",
       "7358  S0038071711004354-1624       2             at             at   \n",
       "7359  S0038071711004354-1624       2             30             30   \n",
       "7360  S0038071711004354-1624       2             cm             cm   \n",
       "7361  S0038071711004354-1624       2       distance       distance   \n",
       "7362  S0038071711004354-1624       2           from           from   \n",
       "7363  S0038071711004354-1624       2            the            the   \n",
       "7364  S0038071711004354-1624       2           dual           dual   \n",
       "7365  S0038071711004354-1624       2              -              -   \n",
       "7366  S0038071711004354-1624       2        chamber        chamber   \n",
       "7367  S0038071711004354-1624       2              .              .   \n",
       "\n",
       "           label  prediction  \n",
       "7328  B-Quantity  B-Quantity  \n",
       "7329  I-Quantity  B-Quantity  \n",
       "7330  I-Quantity  I-Quantity  \n",
       "7331           O           O  \n",
       "7332           O           O  \n",
       "7333           O           O  \n",
       "7334           O           O  \n",
       "7335           O           O  \n",
       "7336           O           O  \n",
       "7337           O           O  \n",
       "7338           O           O  \n",
       "7339           O           O  \n",
       "7340           O           O  \n",
       "7341           O           O  \n",
       "7342           O           O  \n",
       "7343           O           O  \n",
       "7344           O           O  \n",
       "7345           O           O  \n",
       "7346           O           O  \n",
       "7347           O           O  \n",
       "7348           O           O  \n",
       "7349           O           O  \n",
       "7350           O           O  \n",
       "7351           O           O  \n",
       "7352           O           O  \n",
       "7353           O           O  \n",
       "7354           O           O  \n",
       "7355           O           O  \n",
       "7356           O           O  \n",
       "7357           O           O  \n",
       "7358           O           O  \n",
       "7359  B-Quantity  B-Quantity  \n",
       "7360  I-Quantity  I-Quantity  \n",
       "7361           O           O  \n",
       "7362           O           O  \n",
       "7363           O           O  \n",
       "7364           O           O  \n",
       "7365           O           O  \n",
       "7366           O           O  \n",
       "7367           O           O  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['prediction'] = predictions_to_labels(predictions, x_test, labels)\n",
    "test_data[(test_data.docId == \"S0038071711004354-1624\") & (test_data.sentId == 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Although it is not the usual way of evaluating Sequence Labeling tasks, **MeasEval** uses a metric based on the Reading Comprehension *Macro-Averaged F1*. This metric measures the amount of overlapping tokens between the predictions and the true labels. In this assignment, we will approximate this metric by evaluating how many tokens belonging to a *Quantity* are captured by the model. This is done by the `evaluate` function. For the model trained above, the result of this evaluation should look like:\n",
    "\n",
    "> <pre>\n",
    ">               precision    recall  f1-score   support\n",
    "> \n",
    ">     Quantity       0.86      0.73      0.79      1263\n",
    "> \n",
    ">    micro avg       0.86      0.73      0.79      1263\n",
    ">    macro avg       0.86      0.73      0.79      1263\n",
    "> weighted avg       0.86      0.73      0.79      1263\n",
    "> </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    labels = data.apply(lambda x: x['label'].replace(\"B-\", \"\").replace(\"I-\", \"\"), axis=1).values\n",
    "    predictions = data.apply(lambda x: x['prediction'].replace(\"B-\", \"\").replace(\"I-\", \"\"), axis=1).values\n",
    "    print(classification_report(labels, predictions, labels=[\"Quantity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Quantity       0.86      0.73      0.79      1263\n",
      "\n",
      "   micro avg       0.86      0.73      0.79      1263\n",
      "   macro avg       0.86      0.73      0.79      1263\n",
      "weighted avg       0.86      0.73      0.79      1263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Pre-trained Word Embeddings\n",
    "\n",
    "Initializing neural networks with pre-trained word embeddings has a significant impact on many NLP tasks. In the following exercise, you will experiment whether this is also the case for *Quantity* recognition using **GloVe**. You can refer to the following tutorial to learn how to complete this exercise with **keras**: [Using pre-trained word embeddings\n",
    "](https://keras.io/examples/nlp/pretrained_word_embeddings/)\n",
    "\n",
    "\n",
    "First, the `load_embeddings` function will load **GloVe** and return a dictionary mapping words to their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def load_embeddings(glove_path):\n",
    "    embedding_index = {}\n",
    "    with open(glove_path, encoding=\"utf8\") as glove_file:\n",
    "        for line in glove_file:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embedding_index[word] = coefs\n",
    "    return embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove/glove.6B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m glove_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglove/glove.6B.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124md.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m embedding_index \u001b[38;5;241m=\u001b[39m \u001b[43mload_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglove_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[76], line 3\u001b[0m, in \u001b[0;36mload_embeddings\u001b[1;34m(glove_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_embeddings\u001b[39m(glove_path):\n\u001b[0;32m      2\u001b[0m     embedding_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mglove_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m glove_file:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m glove_file:\n\u001b[0;32m      5\u001b[0m             word, coefs \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove/glove.6B.300d.txt'"
     ]
    }
   ],
   "source": [
    "glove_path = f\"glove/glove.6B.{embedding_dim}d.txt\"\n",
    "embedding_index = load_embeddings(glove_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To initialize the `Embedding` layer with the **GloVe** embeddings, you have to create a matrix with `(vocab_size, embedding_dim)` dimensions. The *i-th* row in the matrix corresponds to the *i-th* lemma in the vocabulary and contains the **GloVe** embedding for that lemma.\n",
    "\n",
    "You must complete the code for the `create_embedding_matrix` function. The function takes the embedding dictionary created by `load_embeddings`, the vocabulary dictionary, the size of the vocabulary and the `embedding_dim` hyperparameter. The function should initialize a `(vocab_size, embedding_dim)` **numpy** array with zeros and then replace each row with the appropriate **GloVe** embedding if the corresponding lemma exists in the embedding dictionary. For example, the embedding for \"*statistic*\" should exist in the resulting `embedding_matrix`: \n",
    "> <pre>\n",
    "> array([ 0.1085   ,  0.82802  ,  0.10672  ,  0.0094136, -0.30441  ,\n",
    ">         0.75618  , -0.14705  , -0.15469  , -0.97372  , -0.60413  , ...,\n",
    ">         0.0066196,  0.17067  ,  0.16759  ,  0.63236  ,  0.22037  ,\n",
    ">        -0.2467   , -0.20493  , -0.55613  ,  0.013659 , -0.35642  ])\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create_embedding_matrix"
    ]
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim):\n",
    "    # Initialize the embedding matrix with zeros\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    # Fill the matrix with embeddings from the embedding index\n",
    "    for word, idx in word2idx.items():\n",
    "        if word in embedding_index:\n",
    "            embedding_matrix[idx] = embedding_index[word]\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim)\n",
    "with np.printoptions(linewidth=75, precision=7, edgeitems=10, threshold=10):\n",
    "    display(embedding_matrix[word2idx[\"statistic\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Finally, you can create a new model and load the pre-trained word embedding matrix into the `Embedding` layer.\n",
    "\n",
    "You must complete the code for the `create_model_with_embeddings` function. This function takes as input the size of the vocabulary, the number of labels, the `maxlen`, `embedding_dim` and `rnn_units` hyperparameters, and the embedding matrix created by `create_embedding_matrix`. The function should construct, compile and return a RNN equal to the one built in `create_model` with the only difference that the `Embedding` layer must be initialized with the embedding matrix. Use the [Constant](https://keras.io/api/layers/initializers/) initializer for this purpose. For this task, the Embedding layer must be kept trainable so the embeddings can be updated during training.\n",
    "\n",
    "The initialized weights of the `Embedding` layer for this version of the RNN should look like:\n",
    "> <pre>\n",
    "> array([[ 0.      ,  0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,  0.      ,  0.      ],\n",
    ">        [ 0.      ,  0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,  0.      ,  0.      ],\n",
    ">        [ 0.72004 ,  0.80955 ,  0.7717  , -0.10769 , ..., -0.15423 ,  0.39351 , -0.47083 , -0.60759 ],\n",
    ">        [-0.33848 ,  0.42841 , -0.10284 , -0.22054 , ...,  0.04933 , -0.79888 , -0.41967 , -0.14039 ],\n",
    ">        ...,\n",
    ">        [ 0.1593  ,  0.30671 ,  0.093787,  0.20868 , ...,  1.0097  , -0.76431 , -0.12928 ,  0.29616 ],\n",
    ">        [ 0.      ,  0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,  0.      ,  0.      ],\n",
    ">        [-0.40124 , -0.27991 , -0.42446 , -0.96387 , ...,  0.4937  ,  0.45576 ,  0.61864 , -0.30489 ],\n",
    ">        [ 0.      ,  0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,  0.      ,  0.      ]],\n",
    ">       dtype=float32)\n",
    "> </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create_model_with_embeddings"
    ]
   },
   "outputs": [],
   "source": [
    "def create_model_with_embeddings(vocab_size, label_size, maxlen, embedding_dim, rnn_units, embedding_matrix):\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=maxlen,\n",
    "        mask_zero=True,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        trainable=True\n",
    "    )\n",
    "\n",
    "    model.add(Input(shape=(maxlen,)))\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(units=rnn_units, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(units=label_size, activation='softmax')))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "model_with_embeddings = create_model_with_embeddings(vocab_size, label_size, maxlen, embedding_dim, rnn_units, embedding_matrix)\n",
    "with np.printoptions(linewidth=110, precision=7, edgeitems=4):\n",
    "    display(model_with_embeddings.get_layer(index=0).get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Initializing the `Embedding` layer with **GloVe** embeddings should have a positive impact on the model performance for *Quantity* recognition.\n",
    "\n",
    "> <pre>\n",
    ">               precision    recall  f1-score   support\n",
    "> \n",
    ">     Quantity       0.85      0.79      0.82      1263\n",
    "> \n",
    ">    micro avg       0.85      0.79      0.82      1263\n",
    ">    macro avg       0.85      0.79      0.82      1263\n",
    "> weighted avg       0.85      0.79      0.82      1263\n",
    "> </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "train_model(model_with_embeddings, x_train, y_train, x_dev, y_dev, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "predictions_with_embeddings = make_predictions(model_with_embeddings, x_test, batch_size)\n",
    "test_data['prediction'] = predictions_to_labels(predictions_with_embeddings, x_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
