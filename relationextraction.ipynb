{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Relation Extraction\n",
    "\n",
    "In this assignment, you will work on the [MeasEval](https://competitions.codalab.org/competitions/25770) shared task that was part of SemEval-2021. The goal of **MeasEval** is  the extraction of counts, measurements, and related context from scientific documents. The task is a complex problem that involves solving a number of steps that range from identifying quantities and units of measurement to identify relationships between them. For this assignment, you will focus only on extracting the *HasQuantity* relation:\n",
    "\n",
    " * Given a sentence and the entities it contains, identify the *HasQuantity* relations that link *Quantity* entities with entities of other types. The task can be addressed as Pairwise Relation Extraction.\n",
    "\n",
    "For example, the sentence in the image below contains 1 *Quantity* entity and 2 entities of other types, but only one of the latter is linked to the *Quantity* through the *HasQuantity* relation.\n",
    "\n",
    "![image](img/hasquantity.png)\n",
    "\n",
    "You will develop a Convolutional Neural Network with [keras](https://keras.io/), a high-level Deep Learning API written in **Python** that provides a user-friendly interface for the [TensorFlow](https://www.tensorflow.org/) library, one of the most popular low-level Deep Learning frameworks. You will use the following objects and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, TimeDistributed, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When working with Neural Networks, there are a large number of random operations such as initializing the weights of the network, shuffling the data for training, or choosing samples. This causes that different training runs of the same model can lead to different results. To ensure reproducibility, i.e. obtaining the same results in the different runs, the random number generator must be initialized with a fixed value known as seed. In **keras**, this can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When developing a model, if the results you get are not as expected, try re-initializing the seed by running the cell above before compiling and training the model.\n",
    "\n",
    "> **Note!** With models as complex as Neural Networks, reproducibility is susceptible to factors such as software versions or the hardware on which the models are run. Even with seed initialization, there may be slight differences in the results.\n",
    "\n",
    "Working with Neural Networks also involves defining a number of hyperparameters that set the configuration of the model. Finding the appropriate hyperparameter values requires training the model with different combinations and testing them on the development set. This hyperparameter tuning is a costly process that needs multiple rounds of experimentation. However, for this assignments, you will use the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "maxlen = 130  # Maximum length of the input sequence accepted by the model\n",
    "epochs = 10  # Number of epochs to train the model\n",
    "batch_size = 64  # Number of examples used per gradient update\n",
    "embedding_dim = 300  # Dimension of the embeddings\n",
    "filters = 100  # Number of output filters in the convolution\n",
    "kernel_size = 5  # Length of the convolution window\n",
    "hidden_dim = 10  # Dimension of the hidden dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Training a Deep Learning model with a large train set can be a time-consuming process, as the model needs to iterate over the entire set multiple times, often requiring significant computational resources. During the implementation of the model, it is often a good practice to use only a subset of the training data. This allows a faster debugging of the code. Set the `shrink_dataset` variable as `True` when a faster training is required and set it as `False` to train the model on the whole train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrink_dataset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Although the value of this variable does not affect the tests that will evaluate your code, the output examples distributed throughout this notebook are based on a `shrink_dataset` variable set as `False`.\n",
    "\n",
    "The train set for the assignment consists of 248 articles with 1366 sentences in total. The test set contains 136 articles with 848 sentences. A development set with 65 documents and 459 sentences is also provided. The dataset includes all the entities annotated at the token level following a BIO schema for 4 different types: *MeasuredEntity*, *MeasuredProperty*, *Qualifier* and *Quantity*. Every entity in the dataset has a unique identifier `annotId`. This identifiers are used to indicate if an entity is linked to a *Quantity* with a *HasQuantity* relation. For example, the annotation for a relation between an \"*involved*\" *MeasuredProperty* and a \"*two beach materials*\" *Quantity* would look as follows:\n",
    "\n",
    "|       | docId                  |   sentId | word        | lemma      | label              | annotId   | rel   |\n",
    "|------:|:-----------------------|---------:|:------------|:-----------|:-------------------|:----------|:------|\n",
    "| 22278 | S0378383912000130-3601 |        3 | involved    | involve    | B-MeasuredProperty | T3-1      | T1-1  |\n",
    "| 22279 | S0378383912000130-3601 |        3 | two         | two        | B-Quantity         | T1-1      | nan   |\n",
    "| 22280 | S0378383912000130-3601 |        3 | beach       | beach      | I-Quantity         | T1-1      | nan   |\n",
    "| 22281 | S0378383912000130-3601 |        3 | materials   | material   | I-Quantity         | T1-1      | nan   |\n",
    "\n",
    "The dataset can be loaded into three `DataFrames` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, shrink_dataset, seed):\n",
    "    data = pd.read_csv(data_path, sep=\"\\t\", encoding=\"utf8\")\n",
    "    if shrink_dataset:\n",
    "        sample = data[[\"docId\",  \"sentId\"]].drop_duplicates().sample(frac=0.2, random_state=seed)\n",
    "        data = pd.merge(data, sample, on=[\"docId\", \"sentId\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>sentId</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>label</th>\n",
       "      <th>annotId</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22276</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22277</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>experiments</td>\n",
       "      <td>experiment</td>\n",
       "      <td>B-MeasuredEntity</td>\n",
       "      <td>T4-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22278</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>involved</td>\n",
       "      <td>involve</td>\n",
       "      <td>B-MeasuredProperty</td>\n",
       "      <td>T3-1</td>\n",
       "      <td>T1-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22279</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>B-Quantity</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22280</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>beach</td>\n",
       "      <td>beach</td>\n",
       "      <td>I-Quantity</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22281</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>materials</td>\n",
       "      <td>material</td>\n",
       "      <td>I-Quantity</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22282</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22283</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>nominal</td>\n",
       "      <td>nominal</td>\n",
       "      <td>B-MeasuredProperty</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>T1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22284</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>sediment</td>\n",
       "      <td>sediment</td>\n",
       "      <td>I-MeasuredProperty</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>T1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22285</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>diameters</td>\n",
       "      <td>diameter</td>\n",
       "      <td>I-MeasuredProperty</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>T1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22286</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22287</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>B-Quantity</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22288</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>mm</td>\n",
       "      <td>mm</td>\n",
       "      <td>I-Quantity</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22289</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>I-Quantity</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22290</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>I-Quantity</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22291</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>mm</td>\n",
       "      <td>mm</td>\n",
       "      <td>I-Quantity</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22292</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        docId  sentId         word       lemma  \\\n",
       "22276  S0378383912000130-3601       3          The         the   \n",
       "22277  S0378383912000130-3601       3  experiments  experiment   \n",
       "22278  S0378383912000130-3601       3     involved     involve   \n",
       "22279  S0378383912000130-3601       3          two         two   \n",
       "22280  S0378383912000130-3601       3        beach       beach   \n",
       "22281  S0378383912000130-3601       3    materials    material   \n",
       "22282  S0378383912000130-3601       3         with        with   \n",
       "22283  S0378383912000130-3601       3      nominal     nominal   \n",
       "22284  S0378383912000130-3601       3     sediment    sediment   \n",
       "22285  S0378383912000130-3601       3    diameters    diameter   \n",
       "22286  S0378383912000130-3601       3           of          of   \n",
       "22287  S0378383912000130-3601       3          1.5         1.5   \n",
       "22288  S0378383912000130-3601       3           mm          mm   \n",
       "22289  S0378383912000130-3601       3          and         and   \n",
       "22290  S0378383912000130-3601       3          8.5         8.5   \n",
       "22291  S0378383912000130-3601       3           mm          mm   \n",
       "22292  S0378383912000130-3601       3            .           .   \n",
       "\n",
       "                    label annotId   rel  \n",
       "22276                   O     NaN   NaN  \n",
       "22277    B-MeasuredEntity    T4-1   NaN  \n",
       "22278  B-MeasuredProperty    T3-1  T1-1  \n",
       "22279          B-Quantity    T1-1   NaN  \n",
       "22280          I-Quantity    T1-1   NaN  \n",
       "22281          I-Quantity    T1-1   NaN  \n",
       "22282                   O     NaN   NaN  \n",
       "22283  B-MeasuredProperty    T3-2  T1-2  \n",
       "22284  I-MeasuredProperty    T3-2  T1-2  \n",
       "22285  I-MeasuredProperty    T3-2  T1-2  \n",
       "22286                   O     NaN   NaN  \n",
       "22287          B-Quantity    T1-2   NaN  \n",
       "22288          I-Quantity    T1-2   NaN  \n",
       "22289          I-Quantity    T1-2   NaN  \n",
       "22290          I-Quantity    T1-2   NaN  \n",
       "22291          I-Quantity    T1-2   NaN  \n",
       "22292                   O     NaN   NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_data(\"data/train_rels.tsv\", shrink_dataset, seed)\n",
    "dev_data = load_data(\"data/trial_rels.tsv\", shrink_dataset, seed)\n",
    "test_data = load_data(\"data/eval_rels.tsv\", shrink_dataset, seed)\n",
    "train_data[(train_data.docId == \"S0378383912000130-3601\") & (train_data.sentId == 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The `DataFrames` created include the lemmatization of words in the `lemma` columns. You will use the lemmas as the input of the model.\n",
    "\n",
    "> **Note!** The notebook for this assignment provides very little guidance. You are expected to refer to the [documentation](https://keras.io/api/) for details on how to solve the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "In this assignment, you will have to implement some steps to pre-process and obtain a representation of the data. You will implement a model with an `Embedding` lookup table as the input layer, so the tokens of the input sentences should be represented as indexes. Besides, as one would expect, the sentences in the **MeasEval** dataset have different lengths. However, the input for a Deep Learning model is a batch of examples (in this case, sentences) in the form of a single tensor which requires that all examples in the batch must have the same length. Therefore, the sentences should be padded or truncated to a specific length.\n",
    "\n",
    "> **Note!** For this particular task, the `maxlen` value provided to you guarantees that padding is sufficient to make all sentences the same length without the need for truncation. \n",
    "\n",
    "This first of these pre-processing steps will be to obtain both a vocabulary from the train set. The vocabulary should be the list of unique lemmas and must include the special tokens `[PAD]`, that will be used for padding the sequences, and `[UNK]`, that will be used to represent out-of-vocabulary words. The vocabulary must also include the following set of special tokens that will be use later to tag entities:\n",
    "\n",
    "> &lt;MeasuredEntity&gt;, &lt;/MeasuredEntity&gt;, &lt;MeasuredProperty&gt;, &lt;/MeasuredProperty&gt;, &lt;Qualifier&gt;, &lt;/Qualifier&gt;, &lt;Quantity&gt;, &lt;/Quantity&gt;\n",
    "\n",
    "Along with the vocabulary, you will also have to build a dictionary mapping each lemma to its position in the vocabulary. The dictionary will be used later to obtain the representation of the input of the model. The text is already tokenized and lemmatized which will help in this task.  The resulting vocabulary should should include the special tokens `[PAD]` and `[UNK]` in the first two positions and have 5517 items in total:\n",
    "> <pre>\n",
    "Vocabulary size: 5517\n",
    "Vocabulary first 5 words: ['[PAD]', '[UNK]', '&lt;MeasuredEntity&gt;', '&lt;/MeasuredEntity&gt;', '&lt;MeasuredProperty&gt;', '&lt;/MeasuredProperty&gt;', '&lt;Qualifier&gt;', '&lt;/Qualifier&gt;', '&lt;Quantity&gt;', '&lt;/Quantity&gt;', 'datum', 'be', 'draw', 'from', 'the']\n",
    "Vocabulary dictionary: {'[PAD]': 0, '[UNK]': 1, '&lt;MeasuredEntity>': 2, '&lt;/MeasuredEntity&gt;': 3, '&lt;MeasuredProperty&gt;': 4, '&lt;/MeasuredProperty&gt;': 5, '&lt;Qualifier&gt;': 6, '&lt;/Qualifier&gt;': 7, '&lt;Quantity&gt;': 8, '&lt;/Quantity&gt;': 9, 'datum': 10, 'be': 11, 'draw': 12, 'from': 13, 'the': 14}\n",
    "</pre> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "get_vocabulary"
    ]
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(train_data):\n",
    "    # Initialize vocabulary with required special tokens\n",
    "    vocab = [\n",
    "        '[PAD]', '[UNK]', '<MeasuredEntity>', '</MeasuredEntity>', '<MeasuredProperty>', '</MeasuredProperty>',\n",
    "        '<Qualifier>', '</Qualifier>', '<Quantity>', '</Quantity>'\n",
    "    ]\n",
    "\n",
    "    word2idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "    unique_lemmas = list(set(train_data['lemma'].unique()))\n",
    "    vocab.extend(unique_lemmas)\n",
    "    word2idx.update({lemma: idx for idx, lemma in enumerate(vocab[len(word2idx):], start=len(word2idx))})\n",
    "    \n",
    "    return vocab, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5517\n",
      "Vocabulary first 5 words: ['[PAD]', '[UNK]', '<MeasuredEntity>', '</MeasuredEntity>', '<MeasuredProperty>', '</MeasuredProperty>', '<Qualifier>', '</Qualifier>', '<Quantity>', '</Quantity>', 'isotopic', 'pseudo', 'purpose', 'zno', 'child']\n",
      "Vocabulary dictionary: {'[PAD]': 0, '[UNK]': 1, '<MeasuredEntity>': 2, '</MeasuredEntity>': 3, '<MeasuredProperty>': 4, '</MeasuredProperty>': 5, '<Qualifier>': 6, '</Qualifier>': 7, '<Quantity>': 8, '</Quantity>': 9, 'isotopic': 10, 'pseudo': 11, 'purpose': 12, 'zno': 13, 'child': 14}\n"
     ]
    }
   ],
   "source": [
    "vocab, word2idx, = get_vocabulary(train_data)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary first 5 words: {vocab[:15]}\")\n",
    "print(f\"Vocabulary dictionary: { {w: word2idx[w] for w in vocab[:15]}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In this assignment, you will follow a pairwise strategy for the extraction of the *HasQuantity* relation. The task must be approached by forming relation candidates by pairing all the *Quantity* entities in a sentence with the rest of entities in the sentence that belong to other entity types. For each of this pairs, the input for the model should be the text of the sentence with the entities of the pair marked using the special tags of their corresponding entity type. For example, the input for the relation candidate involving the *Quantity* \"*involved*\" and the *MeasuredProperty* \"*two beach materials*\" would look like this:\n",
    "\n",
    "> The experiment &lt;MeasuredProperty&gt; involved &lt;/MeasuredProperty&gt; &lt;Quantity&gt; two beach materials &lt;/Quantity&gt; with nominal sediment diameter of 1.5 mm and 8.5 mm.\n",
    "\n",
    "This strategy should result in the following number of relation candidates for each data split:\n",
    "\n",
    ">Number of training relation candidates: 2773  \n",
    "Number of development relation candidates: 797  \n",
    "Number of testing relation candidates: 1445\n",
    "\n",
    "You must reformat the train, development and test `DataFrames` by aggregating the data corresponding to each sentence. The input for the model must be the sequence of lemmas in the sentence and the output a binary True/False label indicating whether the entities of the relation candidate hold a *HasQuantity* relation.  The output of `integrate_sentences` must be a `DataFrame` with a row for each relation candidate and the following columns:\n",
    "\n",
    " * `docId`: the `docId` of the relation candidate\n",
    " * `sentId`: the `sentId` of the relation candidate\n",
    " * `quantityId`: the `annotId` of the *Quantity* entity of the pair\n",
    " * `otherId`: the `annotId` of the other entity of the pair\n",
    " * `lemmas`: the list of the lemmas of the sentence of the relation candidate including the special tags\n",
    " * `label`: a binary label indicating if there is a a *HasQuantity* relation between the entity pair\n",
    " \n",
    "For a sentence with 2 *Quantity* entities and 3 other entities, the `DataFrame` should include 6 rows:\n",
    "\n",
    "|      | docId                  |   sentId | quantityId   | otherId   | lemmas                                                                                                                                                                                                                        | label   |\n",
    "|-----:|:-----------------------|---------:|:-------------|:----------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------|\n",
    "| 1817 | S0378383912000130-3601 |        3 | T1-1         | T4-1      | ['the', '&lt;MeasuredEntity&gt;', 'experiment', '&lt;/MeasuredEntity&gt;', 'involve', '&lt;Quantity&gt;', 'two', 'beach', 'material', '&lt;/Quantity&gt;', 'with', 'nominal', 'sediment', 'diameter', 'of', '1.5', 'mm', 'and', '8.5', 'mm', '.']     | False   |\n",
    "| 1818 | S0378383912000130-3601 |        3 | T1-1         | T3-1      | ['the', 'experiment', '&lt;MeasuredProperty&gt;', 'involve', '&lt;/MeasuredProperty&gt;', '&lt;Quantity&gt;', 'two', 'beach', 'material', '&lt;/Quantity&gt;', 'with', 'nominal', 'sediment', 'diameter', 'of', '1.5', 'mm', 'and', '8.5', 'mm', '.'] | True    |\n",
    "| 1819 | S0378383912000130-3601 |        3 | T1-1         | T3-2      | ['the', 'experiment', 'involve', '&lt;Quantity>', 'two', 'beach', 'material', '&lt;/Quantity&gt;', 'with', '&lt;MeasuredProperty&gt;', 'nominal', 'sediment', 'diameter', '&lt;/MeasuredProperty>', 'of', '1.5', 'mm', 'and', '8.5', 'mm', '.'] | False   |\n",
    "| 1820 | S0378383912000130-3601 |        3 | T1-2         | T4-1      | ['the', '&lt;MeasuredEntity&gt;', 'experiment', '&lt;/MeasuredEntity&gt;', 'involve', 'two', 'beach', 'material', 'with', 'nominal', 'sediment', 'diameter', 'of', '&lt;Quantity>', '1.5', 'mm', 'and', '8.5', 'mm', '&lt;/Quantity&gt;', '.']     | False   |\n",
    "| 1821 | S0378383912000130-3601 |        3 | T1-2         | T3-1      | ['the', 'experiment', '&lt;MeasuredProperty&gt;', 'involve', '&lt;/MeasuredProperty&gt;', 'two', 'beach', 'material', 'with', 'nominal', 'sediment', 'diameter', 'of', '&lt;Quantity>', '1.5', 'mm', 'and', '8.5', 'mm', '&lt;/Quantity&gt;', '.'] | False   |\n",
    "| 1822 | S0378383912000130-3601 |        3 | T1-2         | T3-2      | ['the', 'experiment', 'involve', 'two', 'beach', 'material', 'with', '&lt;MeasuredProperty&gt;', 'nominal', 'sediment', 'diameter', '&lt;/MeasuredProperty&gt;', 'of', '&lt;Quantity&gt;', '1.5', 'mm', 'and', '8.5', 'mm', '&lt;/Quantity&gt;', '.'] | True    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "sentence_integrate"
    ]
   },
   "outputs": [],
   "source": [
    "def integrate_sentences(data):\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    quantity_entities = data[data[\"label\"].str.contains(\"B-Quantity\")]\n",
    "    other_entities = data[data[\"label\"].str.contains(\"B-MeasuredProperty|B-MeasuredEntity|B-Qualifier\")]\n",
    "    quantity_entities = quantity_entities.rename(columns={\"annotId\": \"quantityId\", \"rel\": \"quantityRel\", \"lemma\": \"quantityLemma\"}).reset_index()\n",
    "    other_entities = other_entities.rename(columns={\"annotId\": \"otherId\", \"label\": \"otherLabel\", \"lemma\": \"otherLemma\"}).reset_index()\n",
    "\n",
    "    # Join on docId and sentId to get all pairs within each sentence\n",
    "    pairs = pd.merge(quantity_entities, other_entities, on=[\"docId\", \"sentId\"])\n",
    "\n",
    "    # Iterate through each pair to determine label based on the relationship\n",
    "    for _, pair in pairs.iterrows():\n",
    "        label = pair[\"rel\"] == pair[\"quantityId\"]\n",
    "\n",
    "        # Extract all lemmas for the sentence with token indices\n",
    "        sentence_data = data[(data[\"docId\"] == pair[\"docId\"]) & (data[\"sentId\"] == pair[\"sentId\"])].reset_index()\n",
    "        sentence_lemmas = sentence_data[\"lemma\"].tolist()\n",
    "\n",
    "        # Tag Quantity entity in the sentence\n",
    "        start_idx = sentence_data[sentence_data[\"index\"] == pair[\"index_x\"]].index[0]\n",
    "        end_idx = start_idx\n",
    "        while end_idx + 1 < len(sentence_data) and sentence_data.iloc[end_idx + 1][\"label\"].endswith(\"Quantity\"):\n",
    "            end_idx += 1\n",
    "        sentence_lemmas[start_idx] = f\"<Quantity> {sentence_lemmas[start_idx]}\"\n",
    "        sentence_lemmas[end_idx] = f\"{sentence_lemmas[end_idx]} </Quantity>\"\n",
    "\n",
    "        # Tag Other entity in the sentence\n",
    "        start_idx = sentence_data[sentence_data[\"index\"] == pair[\"index_y\"]].index[0]\n",
    "        end_idx = start_idx\n",
    "        while end_idx + 1 < len(sentence_data) and sentence_data.iloc[end_idx + 1][\"label\"].endswith(pair[\"otherLabel\"].split(\"-\")[1]):\n",
    "            end_idx += 1\n",
    "        label_type = pair[\"otherLabel\"].split(\"-\")[1]\n",
    "        sentence_lemmas[start_idx] = f\"<{label_type}> {sentence_lemmas[start_idx]}\"\n",
    "        sentence_lemmas[end_idx] = f\"{sentence_lemmas[end_idx]} </{label_type}>\"\n",
    "\n",
    "        # Ensure separate tags in lemmas\n",
    "        sentence_lemmas = [token for phrase in sentence_lemmas for token in phrase.split()]\n",
    "\n",
    "        # Add the row for this pair\n",
    "        row = {\n",
    "            \"docId\": pair[\"docId\"],\n",
    "            \"sentId\": pair[\"sentId\"],\n",
    "            \"quantityId\": pair[\"quantityId\"],\n",
    "            \"otherId\": pair[\"otherId\"],\n",
    "            \"lemmas\": sentence_lemmas,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    # Drop exact duplicates in all columns to retain unique pairs\n",
    "    results = results.drop_duplicates(subset=[\"docId\", \"sentId\", \"quantityId\", \"otherId\"])\n",
    "    results[\"lemmas\"] = results[\"lemmas\"].apply(tuple)  # Use tuple to ensure drop_duplicates works on lists\n",
    "    results = results.drop_duplicates()\n",
    "    results[\"lemmas\"] = results[\"lemmas\"].apply(list)  # Convert back to list\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def integrate_sentences(data):\n",
    "    relation_candidates = []\n",
    "\n",
    "    grouped = data.groupby(['docId', 'sentId'])\n",
    "\n",
    "    for (doc_id, sent_id), group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "\n",
    "        lemmas = group['lemma'].tolist()\n",
    "\n",
    "        # Separate quantity entities and other entities\n",
    "        quantity_entities = group[group['label'] == \"B-Quantity\"]\n",
    "        other_entities = group[group['label'].isin([\"B-MeasuredProperty\", \"B-MeasuredEntity\", \"B-Qualifier\"])]\n",
    "\n",
    "        for _, quantity_entity in quantity_entities.iterrows():\n",
    "            quantity_annot_id = quantity_entity['annotId']\n",
    "\n",
    "            for _, other_entity in other_entities.iterrows():\n",
    "                other_annot_id = other_entity['annotId']\n",
    "\n",
    "                lemmas_with_tags = lemmas.copy()\n",
    "\n",
    "                # Quantity entity\n",
    "                quantity_rows = group[group['annotId'] == quantity_annot_id]\n",
    "                if not quantity_rows.empty:\n",
    "                    start_idx = quantity_rows.index[0]\n",
    "                    end_idx = start_idx\n",
    "                    while (\n",
    "                        end_idx + 1 < len(group)\n",
    "                        and group.iloc[end_idx + 1][\"label\"].endswith(\"Quantity\")\n",
    "                    ):\n",
    "                        end_idx += 1\n",
    "                    lemmas_with_tags[start_idx] = f\"<Quantity> {lemmas_with_tags[start_idx]}\"\n",
    "                    lemmas_with_tags[end_idx] = f\"{lemmas_with_tags[end_idx]} </Quantity>\"\n",
    "                else:\n",
    "                    print(f\"Warning: Quantity annotId {quantity_annot_id} not found in group.\")\n",
    "\n",
    "                # Other entity\n",
    "                other_rows = group[group['annotId'] == other_annot_id]\n",
    "                if not other_rows.empty:\n",
    "                    start_idx = other_rows.index[0]\n",
    "                    end_idx = start_idx\n",
    "                    label_type = other_entity['label'].split(\"-\")[1]\n",
    "                    while (\n",
    "                        end_idx + 1 < len(group)\n",
    "                        and group.iloc[end_idx + 1][\"label\"].endswith(label_type)\n",
    "                    ):\n",
    "                        end_idx += 1\n",
    "                    lemmas_with_tags[start_idx] = f\"<{label_type}> {lemmas_with_tags[start_idx]}\"\n",
    "                    lemmas_with_tags[end_idx] = f\"{lemmas_with_tags[end_idx]} </{label_type}>\"\n",
    "                else:\n",
    "                    print(f\"Warning: Other annotId {other_annot_id} not found in group.\")\n",
    "\n",
    "                # Flatten nested tags into single tokens\n",
    "                lemmas_with_tags = ' '.join(lemmas_with_tags).split()\n",
    "                # Apply consistent format for each token in lemmas_with_tags\n",
    "                lemmas_with_tags = [token for token in lemmas_with_tags]\n",
    "\n",
    "                # HasQuantity relation\n",
    "                label = (quantity_entity['annotId'] == other_entity['rel'])\n",
    "\n",
    "                relation_candidates.append({\n",
    "                    'docId': doc_id,\n",
    "                    'sentId': sent_id,\n",
    "                    'quantityId': quantity_annot_id,\n",
    "                    'otherId': other_annot_id,\n",
    "                    'lemmas': lemmas_with_tags,\n",
    "                    'label': label\n",
    "                })\n",
    "\n",
    "    relation_candidates = pd.DataFrame(relation_candidates)\n",
    "    # Drop exact duplicates in all columns to retain unique pairs\n",
    "    relation_candidates = relation_candidates.drop_duplicates(subset=['docId', 'sentId', 'quantityId', 'otherId'])\n",
    "    relation_candidates['lemmas'] = relation_candidates['lemmas'].apply(tuple)  # Use tuple to ensure drop_duplicates works on lists\n",
    "    relation_candidates = relation_candidates.drop_duplicates()\n",
    "    relation_candidates['lemmas'] = relation_candidates['lemmas'].apply(list)  # Convert back to list\n",
    "\n",
    "    return relation_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>sentId</th>\n",
       "      <th>quantityId</th>\n",
       "      <th>otherId</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>T4-1</td>\n",
       "      <td>[the, &lt;MeasuredEntity&gt;, experiment, &lt;/MeasuredEntity&gt;, involve, &lt;Quantity&gt;, two, beach, material, &lt;/Quantity&gt;, with, nominal, sediment, diameter, of, 1.5, mm, and, 8.5, mm, .]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>T3-1</td>\n",
       "      <td>[the, experiment, &lt;MeasuredProperty&gt;, involve, &lt;/MeasuredProperty&gt;, &lt;Quantity&gt;, two, beach, material, &lt;/Quantity&gt;, with, nominal, sediment, diameter, of, 1.5, mm, and, 8.5, mm, .]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>[the, experiment, involve, &lt;Quantity&gt;, two, beach, material, &lt;/Quantity&gt;, with, &lt;MeasuredProperty&gt;, nominal, sediment, diameter, &lt;/MeasuredProperty&gt;, of, 1.5, mm, and, 8.5, mm, .]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>T4-1</td>\n",
       "      <td>[the, &lt;MeasuredEntity&gt;, experiment, &lt;/MeasuredEntity&gt;, involve, two, beach, material, with, nominal, sediment, diameter, of, &lt;Quantity&gt;, 1.5, mm, and, 8.5, mm, &lt;/Quantity&gt;, .]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>T3-1</td>\n",
       "      <td>[the, experiment, &lt;MeasuredProperty&gt;, involve, &lt;/MeasuredProperty&gt;, two, beach, material, with, nominal, sediment, diameter, of, &lt;Quantity&gt;, 1.5, mm, and, 8.5, mm, &lt;/Quantity&gt;, .]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>S0378383912000130-3601</td>\n",
       "      <td>3</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>[the, experiment, involve, two, beach, material, with, &lt;MeasuredProperty&gt;, nominal, sediment, diameter, &lt;/MeasuredProperty&gt;, of, &lt;Quantity&gt;, 1.5, mm, and, 8.5, mm, &lt;/Quantity&gt;, .]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       docId  sentId quantityId otherId  \\\n",
       "1817  S0378383912000130-3601       3       T1-1    T4-1   \n",
       "1818  S0378383912000130-3601       3       T1-1    T3-1   \n",
       "1819  S0378383912000130-3601       3       T1-1    T3-2   \n",
       "1820  S0378383912000130-3601       3       T1-2    T4-1   \n",
       "1821  S0378383912000130-3601       3       T1-2    T3-1   \n",
       "1822  S0378383912000130-3601       3       T1-2    T3-2   \n",
       "\n",
       "                                                                                                                                                                                   lemmas  \\\n",
       "1817      [the, <MeasuredEntity>, experiment, </MeasuredEntity>, involve, <Quantity>, two, beach, material, </Quantity>, with, nominal, sediment, diameter, of, 1.5, mm, and, 8.5, mm, .]   \n",
       "1818  [the, experiment, <MeasuredProperty>, involve, </MeasuredProperty>, <Quantity>, two, beach, material, </Quantity>, with, nominal, sediment, diameter, of, 1.5, mm, and, 8.5, mm, .]   \n",
       "1819  [the, experiment, involve, <Quantity>, two, beach, material, </Quantity>, with, <MeasuredProperty>, nominal, sediment, diameter, </MeasuredProperty>, of, 1.5, mm, and, 8.5, mm, .]   \n",
       "1820      [the, <MeasuredEntity>, experiment, </MeasuredEntity>, involve, two, beach, material, with, nominal, sediment, diameter, of, <Quantity>, 1.5, mm, and, 8.5, mm, </Quantity>, .]   \n",
       "1821  [the, experiment, <MeasuredProperty>, involve, </MeasuredProperty>, two, beach, material, with, nominal, sediment, diameter, of, <Quantity>, 1.5, mm, and, 8.5, mm, </Quantity>, .]   \n",
       "1822  [the, experiment, involve, two, beach, material, with, <MeasuredProperty>, nominal, sediment, diameter, </MeasuredProperty>, of, <Quantity>, 1.5, mm, and, 8.5, mm, </Quantity>, .]   \n",
       "\n",
       "      label  \n",
       "1817  False  \n",
       "1818   True  \n",
       "1819  False  \n",
       "1820  False  \n",
       "1821  False  \n",
       "1822   True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training relation candidates: 2773\n",
      "Number of development relation candidates: 797\n",
      "Number of testing relation candidates: 1445\n"
     ]
    }
   ],
   "source": [
    "train_examples = integrate_sentences(train_data)\n",
    "dev_examples = integrate_sentences(dev_data)\n",
    "test_examples = integrate_sentences(test_data)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(train_examples[(train_examples.docId == \"S0378383912000130-3601\") & (train_examples.sentId == 3)])\n",
    "print(\"Number of training relation candidates: %s\" % len(train_examples))\n",
    "print(\"Number of development relation candidates: %s\" % len(dev_examples))\n",
    "print(\"Number of testing relation candidates: %s\" % len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The dataset is now ready for you to get the numerical representation of the input. You must perform two steps to process the sequence of lemmas:\n",
    "1. For each sentence, translate each lemma or label to its corresponding index using the `word2idx` and `label2idx` dictionaries. In case the lemma is not found in `word2idx`, use the index of the `[UNK]` token instead.\n",
    "2. Pad both the sequences of lemmas and the sequences of labels to the same length as defined by the `maxlen` variable using the index of the `[PAD]` token in the vocabulary.   \n",
    "\n",
    "You must also return the target labels in a separate array.\n",
    "\n",
    "Applying `format_examples` to the train, development and test sets should result on 6 arrays with the following shapes:\n",
    "\n",
    ">Shape of training input data:  (2773, 130)  \n",
    "Shape of training output data:  (2773,)  \n",
    "Shape of development input data:  (797, 130)  \n",
    "Shape of development output data:  (797,)  \n",
    "Shape of testing input data:  (1445, 130)  \n",
    "Shape of testing output data:  (1445,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": [
     "format_examples"
    ]
   },
   "outputs": [],
   "source": [
    "def format_examples(data, word2idx, maxlen):\n",
    "\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for lemmas, labels in zip(data['lemmas'], data['label']):\n",
    "        lemma_indices = [word2idx.get(lemma, word2idx['[UNK]']) for lemma in lemmas]\n",
    "        label_indices = 1 if labels else 0\n",
    "\n",
    "        x_data.append(lemma_indices)\n",
    "        y_data.append(label_indices)\n",
    "\n",
    "    x_data = pad_sequences(x_data, maxlen=maxlen)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training input data:  (2773, 130)\n",
      "Shape of training output data:  (2773,)\n",
      "Shape of development input data:  (797, 130)\n",
      "Shape of development output data:  (797,)\n",
      "Shape of testing input data:  (1445, 130)\n",
      "Shape of testing output data:  (1445,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = format_examples(train_examples, word2idx, maxlen)\n",
    "x_dev, y_dev = format_examples(dev_examples, word2idx, maxlen)\n",
    "x_test, y_test = format_examples(test_examples, word2idx, maxlen)\n",
    "print(\"Shape of training input data: \", x_train.shape)\n",
    "print(\"Shape of training output data: \", y_train.shape)\n",
    "print(\"Shape of development input data: \", x_dev.shape)\n",
    "print(\"Shape of development output data: \", y_dev.shape)\n",
    "print(\"Shape of testing input data: \", x_test.shape)\n",
    "print(\"Shape of testing output data: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "You will construct a CNN as a **keras** `Sequential` model with the following 5 layers:\n",
    "\n",
    "1. An `Input` layer where the length of the input sequences is set to `maxlen`.\n",
    "1. An `Embedding` layer with an input dimension equal to the vocabulary size and an embedding dimension defined by `embedding_dim`.\n",
    "2. A `Conv1D` layer with a number of output filters determined by `filters`, a length of convolution window equal to `kernel_size` and a `relu` activation function.\n",
    "3. A `GlobalMaxPooling1D` to downsample the output of the convolutional layer.\n",
    "4. A `Dense` layer with a number of units equal to `hidden_dim` and a `relu` activation function.\n",
    "5. A `Dense` layer with 1 unit and a `sigmoid` activation function.\n",
    "\n",
    "Any option not mentioned in the description should be kept with its default value. The summary of the resulting model should look like:\n",
    "\n",
    "\n",
    "> <pre>\n",
    "> Model: \"sequential\"\n",
    "> __________________________________________________________________________________________\n",
    ">  Layer (type)                           Output Shape                        Param #       \n",
    "> ==========================================================================================\n",
    ">  embedding (Embedding)                  (None, 130, 300)                    1655100       \n",
    ">                                                                                           \n",
    ">  conv1d (Conv1D)                        (None, 126, 100)                    150100        \n",
    ">                                                                                           \n",
    ">  global_max_pooling1d (GlobalMaxPooling  (None, 100)                        0             \n",
    ">  1D)                                                                                      \n",
    ">                                                                                           \n",
    ">  dense (Dense)                          (None, 10)                          1010          \n",
    ">                                                                                           \n",
    ">  dense_1 (Dense)                        (None, 1)                           11            \n",
    ">                                                                                           \n",
    "> ==========================================================================================\n",
    "> Total params: 1,806,221\n",
    "> Trainable params: 1,806,221\n",
    "> Non-trainable params: 0\n",
    "> __________________________________________________________________________________________\n",
    "> </pre>\n",
    "\n",
    "Before returning the model, you should compile it using `'binary_crossentropy'` as the loss function, `'adam'` as the optimizer and `'binary_accuracy'` as a metric to evaluate the model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": [
     "create_model"
    ]
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, maxlen, embedding_dim, filters, kernel_size, hidden_dim):\n",
    "   # Define model \n",
    "    model = Sequential([\n",
    "        Input(shape=(maxlen,)),\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(hidden_dim, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                          </span>┃<span style=\"font-weight: bold\"> Output Shape                 </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,655,100</span> │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">150,100</span> │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                  │                              │                 │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n",
       "└───────────────────────────────────────┴──────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │       \u001b[38;5;34m1,655,100\u001b[0m │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │         \u001b[38;5;34m150,100\u001b[0m │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                  │                              │                 │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                   │           \u001b[38;5;34m1,010\u001b[0m │\n",
       "├───────────────────────────────────────┼──────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                    │              \u001b[38;5;34m11\u001b[0m │\n",
       "└───────────────────────────────────────┴──────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,806,221</span> (6.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,806,221\u001b[0m (6.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,806,221</span> (6.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,806,221\u001b[0m (6.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_model(vocab_size, maxlen, embedding_dim, filters, kernel_size, hidden_dim)\n",
    "model.summary(line_length=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Once the data has been processed and the model has been compiled, you can proceed to train it using the train input and output obtained by `format_examples` as well as the development input and output produced by the same function. Yould should also use the `batch_size` and `epochs` hyperparameters and evaluate the loss and any model metrics on the development data during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": [
     "train_model"
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_dev, y_dev, batch_size, epochs):\n",
    "    \n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_dev, y_dev),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - binary_accuracy: 0.6578 - loss: 0.6288 - val_binary_accuracy: 0.7792 - val_loss: 0.4734\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - binary_accuracy: 0.7954 - loss: 0.4371 - val_binary_accuracy: 0.8695 - val_loss: 0.3240\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - binary_accuracy: 0.9067 - loss: 0.2441 - val_binary_accuracy: 0.8846 - val_loss: 0.2792\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - binary_accuracy: 0.9500 - loss: 0.1484 - val_binary_accuracy: 0.8858 - val_loss: 0.2804\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - binary_accuracy: 0.9689 - loss: 0.0966 - val_binary_accuracy: 0.8695 - val_loss: 0.3112\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - binary_accuracy: 0.9765 - loss: 0.0682 - val_binary_accuracy: 0.8507 - val_loss: 0.3464\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - binary_accuracy: 0.9812 - loss: 0.0519 - val_binary_accuracy: 0.8745 - val_loss: 0.3355\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - binary_accuracy: 0.9878 - loss: 0.0340 - val_binary_accuracy: 0.8482 - val_loss: 0.4222\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - binary_accuracy: 0.9875 - loss: 0.0317 - val_binary_accuracy: 0.8218 - val_loss: 0.5065\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - binary_accuracy: 0.9887 - loss: 0.0297 - val_binary_accuracy: 0.8369 - val_loss: 0.5033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e7a63e7a10>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, x_train, y_train, x_dev, y_dev, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "After training, you can apply the model to make predictions on  the test input data produced by `format_examples` using the `batch_size` hyperparameter. Since the output layer of the model is a `sigmoid` function, it will return values ranging from 0 to 1. For each prediction, you should obtain a boolean `True` label in case the prediction is higher than `0.5` or `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": [
     "make_predictions"
    ]
   },
   "outputs": [],
   "source": [
    "def make_predictions(model, x_test, batch_size):\n",
    "    \n",
    "    pred = model.predict(x_test, batch_size=batch_size).flatten()\n",
    "    pred_labels = pred > 0.5\n",
    "\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>sentId</th>\n",
       "      <th>quantityId</th>\n",
       "      <th>otherId</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>T1-5</td>\n",
       "      <td>T4-5</td>\n",
       "      <td>[&lt;Quantity&gt;, approximately, 15, min, &lt;/Quantity&gt;, &lt;Qualifier&gt;, before, the, beginning, of, the, experiment, &lt;/Qualifier&gt;, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, 30, cm, distance, from, the, dual, -, chamber, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>T1-5</td>\n",
       "      <td>T2-5</td>\n",
       "      <td>[&lt;Quantity&gt;, approximately, 15, min, &lt;/Quantity&gt;, before, the, beginning, of, the, experiment, ,, the, &lt;MeasuredEntity&gt;, aboveground, vegetation, &lt;/MeasuredEntity&gt;, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, 30, cm, distance, from, the, dual, -, chamber, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>T1-5</td>\n",
       "      <td>T3-5</td>\n",
       "      <td>[&lt;Quantity&gt;, approximately, 15, min, &lt;/Quantity&gt;, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, &lt;MeasuredProperty&gt;, remove, under, the, dual, -, chamber, &lt;/MeasuredProperty&gt;, ,, as, well, as, the, reference, chamber, which, be, deploy, at, 30, cm, distance, from, the, dual, -, chamber, .]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>T1-5</td>\n",
       "      <td>T4-6</td>\n",
       "      <td>[&lt;Quantity&gt;, approximately, 15, min, &lt;/Quantity&gt;, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, &lt;MeasuredEntity&gt;, reference, chamber, &lt;/MeasuredEntity&gt;, which, be, deploy, at, 30, cm, distance, from, the, dual, -, chamber, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>T1-5</td>\n",
       "      <td>T3-6</td>\n",
       "      <td>[&lt;Quantity&gt;, approximately, 15, min, &lt;/Quantity&gt;, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, 30, cm, &lt;MeasuredProperty&gt;, distance, from, the, dual, -, chamber, &lt;/MeasuredProperty&gt;, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>T1-6</td>\n",
       "      <td>T4-5</td>\n",
       "      <td>[approximately, 15, min, &lt;Qualifier&gt;, before, the, beginning, of, the, experiment, &lt;/Qualifier&gt;, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, &lt;Quantity&gt;, 30, cm, &lt;/Quantity&gt;, distance, from, the, dual, -, chamber, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>T1-6</td>\n",
       "      <td>T2-5</td>\n",
       "      <td>[approximately, 15, min, before, the, beginning, of, the, experiment, ,, the, &lt;MeasuredEntity&gt;, aboveground, vegetation, &lt;/MeasuredEntity&gt;, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, &lt;Quantity&gt;, 30, cm, &lt;/Quantity&gt;, distance, from, the, dual, -, chamber, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>T1-6</td>\n",
       "      <td>T3-5</td>\n",
       "      <td>[approximately, 15, min, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, &lt;MeasuredProperty&gt;, remove, under, the, dual, -, chamber, &lt;/MeasuredProperty&gt;, ,, as, well, as, the, reference, chamber, which, be, deploy, at, &lt;Quantity&gt;, 30, cm, &lt;/Quantity&gt;, distance, from, the, dual, -, chamber, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>T1-6</td>\n",
       "      <td>T4-6</td>\n",
       "      <td>[approximately, 15, min, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, &lt;MeasuredEntity&gt;, reference, chamber, &lt;/MeasuredEntity&gt;, which, be, deploy, at, &lt;Quantity&gt;, 30, cm, &lt;/Quantity&gt;, distance, from, the, dual, -, chamber, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>S0038071711004354-1624</td>\n",
       "      <td>2</td>\n",
       "      <td>T1-6</td>\n",
       "      <td>T3-6</td>\n",
       "      <td>[approximately, 15, min, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, &lt;Quantity&gt;, 30, cm, &lt;/Quantity&gt;, &lt;MeasuredProperty&gt;, distance, from, the, dual, -, chamber, &lt;/MeasuredProperty&gt;, .]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docId  sentId quantityId otherId  \\\n",
       "489  S0038071711004354-1624       2       T1-5    T4-5   \n",
       "490  S0038071711004354-1624       2       T1-5    T2-5   \n",
       "491  S0038071711004354-1624       2       T1-5    T3-5   \n",
       "492  S0038071711004354-1624       2       T1-5    T4-6   \n",
       "493  S0038071711004354-1624       2       T1-5    T3-6   \n",
       "494  S0038071711004354-1624       2       T1-6    T4-5   \n",
       "495  S0038071711004354-1624       2       T1-6    T2-5   \n",
       "496  S0038071711004354-1624       2       T1-6    T3-5   \n",
       "497  S0038071711004354-1624       2       T1-6    T4-6   \n",
       "498  S0038071711004354-1624       2       T1-6    T3-6   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                    lemmas  \\\n",
       "489                [<Quantity>, approximately, 15, min, </Quantity>, <Qualifier>, before, the, beginning, of, the, experiment, </Qualifier>, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, 30, cm, distance, from, the, dual, -, chamber, .]   \n",
       "490      [<Quantity>, approximately, 15, min, </Quantity>, before, the, beginning, of, the, experiment, ,, the, <MeasuredEntity>, aboveground, vegetation, </MeasuredEntity>, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, 30, cm, distance, from, the, dual, -, chamber, .]   \n",
       "491  [<Quantity>, approximately, 15, min, </Quantity>, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, <MeasuredProperty>, remove, under, the, dual, -, chamber, </MeasuredProperty>, ,, as, well, as, the, reference, chamber, which, be, deploy, at, 30, cm, distance, from, the, dual, -, chamber, .]   \n",
       "492      [<Quantity>, approximately, 15, min, </Quantity>, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, <MeasuredEntity>, reference, chamber, </MeasuredEntity>, which, be, deploy, at, 30, cm, distance, from, the, dual, -, chamber, .]   \n",
       "493  [<Quantity>, approximately, 15, min, </Quantity>, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, 30, cm, <MeasuredProperty>, distance, from, the, dual, -, chamber, </MeasuredProperty>, .]   \n",
       "494                [approximately, 15, min, <Qualifier>, before, the, beginning, of, the, experiment, </Qualifier>, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, <Quantity>, 30, cm, </Quantity>, distance, from, the, dual, -, chamber, .]   \n",
       "495      [approximately, 15, min, before, the, beginning, of, the, experiment, ,, the, <MeasuredEntity>, aboveground, vegetation, </MeasuredEntity>, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, <Quantity>, 30, cm, </Quantity>, distance, from, the, dual, -, chamber, .]   \n",
       "496  [approximately, 15, min, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, <MeasuredProperty>, remove, under, the, dual, -, chamber, </MeasuredProperty>, ,, as, well, as, the, reference, chamber, which, be, deploy, at, <Quantity>, 30, cm, </Quantity>, distance, from, the, dual, -, chamber, .]   \n",
       "497      [approximately, 15, min, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, <MeasuredEntity>, reference, chamber, </MeasuredEntity>, which, be, deploy, at, <Quantity>, 30, cm, </Quantity>, distance, from, the, dual, -, chamber, .]   \n",
       "498  [approximately, 15, min, before, the, beginning, of, the, experiment, ,, the, aboveground, vegetation, be, remove, under, the, dual, -, chamber, ,, as, well, as, the, reference, chamber, which, be, deploy, at, <Quantity>, 30, cm, </Quantity>, <MeasuredProperty>, distance, from, the, dual, -, chamber, </MeasuredProperty>, .]   \n",
       "\n",
       "     label  prediction  \n",
       "489  False       False  \n",
       "490  False       False  \n",
       "491   True        True  \n",
       "492  False        True  \n",
       "493  False       False  \n",
       "494  False       False  \n",
       "495  False       False  \n",
       "496  False        True  \n",
       "497  False        True  \n",
       "498   True        True  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples['prediction'] = make_predictions(model, x_test, batch_size)\n",
    "test_examples[(test_examples.docId == \"S0038071711004354-1624\") & (test_examples.sentId == 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Since the goal of the task is the extraction of the *HasQuality* relation, the metric used for evaluation should only report results for the `True` cases. For this, `binary f1` can be used. For the model trained above, the result of this evaluation should look like:\n",
    "\n",
    "> binary f1: 0.771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    print(\"binary f1: %.3f\" % f1_score(data['label'], data['prediction'], average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "evaluate(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
